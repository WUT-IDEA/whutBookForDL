{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归神经网络 - Recurrent Neural Network\n",
    "====\n",
    ">Python2.7 + Pytorch 1.2.0 backened\n",
    ">\n",
    ">text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "'''\n",
    "@author: deep learning textbook of whut\n",
    "@date: 2017-10-31\n",
    "'''\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Deep learning (also known as deep structured learning or hierarchical learning)\n",
    "is part of a broader family of machine learning methods based on learning data\n",
    "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
    "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
    "processing and communication patterns in a biological nervous system, such as neural\n",
    "coding that attempts to define a relationship between various stimuli and associated\n",
    "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
    "networks, deep belief networks and recurrent neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing,\n",
    "audio recognition, social network filtering, machine translation, bioinformatics\n",
    "and drug design,[5] where they have produced results comparable to and in some\n",
    "cases superior[6] to human experts.\n",
    "\"\"\".split()\n",
    "# from wikipedia https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "vocab = set(sentence)\n",
    "word2ind = {word: i for i, word in enumerate(vocab)}\n",
    "ind2word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# hyper-parameter\n",
    "input_timesteps = 2\n",
    "output_timesteps = 1\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_size = 60\n",
    "layers_num = 2\n",
    "training_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = len(sentence) - input_timesteps\n",
    "x_data = [[word2ind[ch] for ch in sentence[i:i + input_timesteps]]\n",
    "          for i in xrange(len(sentence) - input_timesteps)]\n",
    "y_data = [[word2ind[sentence[i]]] for i in xrange(input_timesteps, len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.placeholder(dtype=tf.int32, shape=[None, input_timesteps])\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None, output_timesteps])\n",
    "\n",
    "onehot_encoding = lambda tensor: tf.one_hot(tensor, depth=vocab_size, axis=-1)\n",
    "output_tensor = onehot_encoding(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embedding_layer, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN中的Dropout\n",
    "link: https://stackoverflow.com/questions/45917464/tensorflow-whats-the-difference-between-tf-nn-dropout-and-tf-contrib-rnn-dropo<br>\n",
    "tensorflow中有两种Dropout手段：<br>\n",
    "1.`tf.nn.droupout`：以上一个网络的输出的部分作为下一层网络的输入。适用于一切网络。<br>\n",
    "2.`tf.contrib.rnn.DropoutWrapper`：在RNN cell内部实现dropout，可以控制RNN网络的输入和输出dropout。只适用于RNN内部。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中创建多层RNN\n",
    "`tensorflow`中有2种方法可以实现多层RNN：<br>\n",
    "### 1.利用`rnn.MultiRNNCell`和`rnn.static_rnn`/`tf.nn.static_rnn`/`tf.nn.dynamic_rnn`的组合实现<br>\n",
    "#### i).`tf.nn.dynamic_rnn`<br>\n",
    "    不需要拆分<br>\n",
    "``\n",
    "cell = rnn.MultiRNNCell([rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "                             for _ in xrange(num_layers)])\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "``\n",
    "<br>\n",
    "#### ii).`rnn.static_rnn` <=> `tf.nn.static_rnn`<br>\n",
    "需要拆分<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "cell = rnn.MultiRNNCell([rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "                             for _ in xrange(num_layers)])\n",
    "outputs, state = tf.nn.static_rnn/rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "``<br>\n",
    "### 2.通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "#### i).`tf.nn.dynamic_rnn`<br>\n",
    "不需要拆分<br>\n",
    "``\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"rnn\"):\n",
    "        x, state = tf.nn.dynamic_rnn(\n",
    "            rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob),\n",
    "            inputs=x, dtype=tf.float32)\n",
    "outputs = x\n",
    "``<br>\n",
    "#### ii).`rnn.static_rnn` <=> `tf.nn.static_rnn`<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"rnn\"):\n",
    "        x, state = tf.nn.static_rnn(\n",
    "            rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob),\n",
    "            inputs=x, dtype=tf.float32)\n",
    "outputs = tf.concat(x, axis=-1)\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中创建多层RNN和多层BiRNN\n",
    "\n",
    "BiRNN不能像RNN那样灵活，需要控制输入和输出的流程，代码写起来比较冗长；不能直接使用for循环BiRNN，需要使用函数实现多层BiRNN<br>\n",
    "tensorflow中biRNN共有5个接口:<br>\n",
    "### 1.tensorflow.contrib.rnn.stack_bidirectional_rnn(cells_fw, cells_bw, ...)\n",
    "需要`tf.unstack`，将`[batch, timestep, length]`的`timestep`拆分为`list`<br>\n",
    "`cells_fw`, `cells_bw`必须为`list`，`list`的长度为RNN网络层数<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "cell_fw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "cell_bw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "outputs, state_fw, state_bw = rnn.stack_bidirectional_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "outputs = tf.stack(outputs, axis=1)\n",
    "``\n",
    "### 2.tensorflow.contrib.rnn.stack_bidirectional_dynamic_rnn\n",
    "不需要拆分<br>\n",
    "``\n",
    "cell_fw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "cell_bw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "outputs, state_fw, state_bw = rnn.stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "``\n",
    "### 3.tf.nn.bidirectional_dynamic_rnn\n",
    "不需要拆分；通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "``\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"bidirectional-rnn\"):\n",
    "        cell_fw = rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob)\n",
    "        cell_bw = rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob)\n",
    "        x, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "    x = tf.concat(x, axis=-1)\n",
    "outputs = x\n",
    "``\n",
    "### 4.tensorflow.contrib.rnn.static_bidirectional_rnn = 5.tf.nn.static_bidirectional_rnn\n",
    "需要拆分；通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"bidirectional-rnn\"):\n",
    "        cell_fw = rnn.DropoutWrapper(cell(units, activation=activation))\n",
    "        cell_bw = rnn.DropoutWrapper(cell(units, activation=activation))\n",
    "        x, state_fw, state_bw = rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "outputs = tf.stack(x, axis=1)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "def RNN(x, num_hidden,\n",
    "        cell_type=rnn.BasicLSTMCell,\n",
    "        activation=tf.nn.relu,\n",
    "        dropout_prob=1.0,\n",
    "        num_layers=1):\n",
    "    assert cell_type in [rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell], \\\n",
    "        'RNN cell is wrong, must be in \"rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell\", but it is %s.' % (cell_type)\n",
    "    assert type(num_layers) == int and num_layers >= 1\n",
    "    assert 0.0 < dropout_prob <= 1.0\n",
    "\n",
    "    # RNN\n",
    "    def mRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    # BiRNN\n",
    "    def mBiRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    cell_fw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    cell_bw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "mLSTM = RNN(embed, hidden_size, dropout_prob=0.8, num_layers=2)\n",
    "mLSTM = tf.reshape(mLSTM, [-1, output_timesteps, input_timesteps * hidden_size * 2])\n",
    "fc1 = tf.layers.dense(inputs=mLSTM, units=vocab_size, activation=tf.nn.softmax)\n",
    "y_pred = fc1\n",
    "y_pred_max = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "loss_op = tf.losses.softmax_cross_entropy(output_tensor, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 / 10000, training cost: 3.4513\n",
      "Epoch 200 / 10000, training cost: 3.3945\n",
      "Epoch 300 / 10000, training cost: 3.36933\n",
      "Epoch 400 / 10000, training cost: 3.36965\n",
      "Epoch 500 / 10000, training cost: 3.3688\n",
      "Epoch 600 / 10000, training cost: 3.3538\n",
      "Epoch 700 / 10000, training cost: 3.35243\n",
      "Epoch 800 / 10000, training cost: 3.34151\n",
      "Epoch 900 / 10000, training cost: 3.33744\n",
      "Epoch 1000 / 10000, training cost: 3.33535\n",
      "Epoch 1100 / 10000, training cost: 3.33682\n",
      "Epoch 1200 / 10000, training cost: 3.33514\n",
      "Epoch 1300 / 10000, training cost: 3.33557\n",
      "Epoch 1400 / 10000, training cost: 3.33478\n",
      "Epoch 1500 / 10000, training cost: 3.33454\n",
      "Epoch 1600 / 10000, training cost: 3.33479\n",
      "Epoch 1700 / 10000, training cost: 3.33421\n",
      "Epoch 1800 / 10000, training cost: 3.33564\n",
      "Epoch 1900 / 10000, training cost: 3.32918\n",
      "Epoch 2000 / 10000, training cost: 3.32464\n",
      "Epoch 2100 / 10000, training cost: 3.3251\n",
      "Epoch 2200 / 10000, training cost: 3.32479\n",
      "Epoch 2300 / 10000, training cost: 3.32483\n",
      "Epoch 2400 / 10000, training cost: 3.32455\n",
      "Epoch 2500 / 10000, training cost: 3.32393\n",
      "Epoch 2600 / 10000, training cost: 3.32449\n",
      "Epoch 2700 / 10000, training cost: 3.32442\n",
      "Epoch 2800 / 10000, training cost: 3.32434\n",
      "Epoch 2900 / 10000, training cost: 3.32434\n",
      "Epoch 3000 / 10000, training cost: 3.3244\n",
      "Epoch 3100 / 10000, training cost: 3.30746\n",
      "Epoch 3200 / 10000, training cost: 3.30146\n",
      "Epoch 3300 / 10000, training cost: 3.30038\n",
      "Epoch 3400 / 10000, training cost: 3.30103\n",
      "Epoch 3500 / 10000, training cost: 3.29094\n",
      "Epoch 3600 / 10000, training cost: 3.28555\n",
      "Epoch 3700 / 10000, training cost: 3.28554\n",
      "Epoch 3800 / 10000, training cost: 3.28614\n",
      "Epoch 3900 / 10000, training cost: 3.28637\n",
      "Epoch 4000 / 10000, training cost: 3.28634\n",
      "Epoch 4100 / 10000, training cost: 3.28698\n",
      "Epoch 4200 / 10000, training cost: 3.28551\n",
      "Epoch 4300 / 10000, training cost: 3.28497\n",
      "Epoch 4400 / 10000, training cost: 3.28542\n",
      "Epoch 4500 / 10000, training cost: 3.28466\n",
      "Epoch 4600 / 10000, training cost: 3.27977\n",
      "Epoch 4700 / 10000, training cost: 3.27563\n",
      "Epoch 4800 / 10000, training cost: 3.27507\n",
      "Epoch 4900 / 10000, training cost: 3.27389\n",
      "Epoch 5000 / 10000, training cost: 3.27316\n",
      "Epoch 5100 / 10000, training cost: 3.27487\n",
      "Epoch 5200 / 10000, training cost: 3.2739\n",
      "Epoch 5300 / 10000, training cost: 3.27377\n",
      "Epoch 5400 / 10000, training cost: 3.27357\n",
      "Epoch 5500 / 10000, training cost: 3.27419\n",
      "Epoch 5600 / 10000, training cost: 3.27287\n",
      "Epoch 5700 / 10000, training cost: 3.2704\n",
      "Epoch 5800 / 10000, training cost: 3.26919\n",
      "Epoch 5900 / 10000, training cost: 3.27221\n",
      "Epoch 6000 / 10000, training cost: 3.26909\n",
      "Epoch 6100 / 10000, training cost: 3.26739\n",
      "Epoch 6200 / 10000, training cost: 3.26644\n",
      "Epoch 6300 / 10000, training cost: 3.26754\n",
      "Epoch 6400 / 10000, training cost: 3.26778\n",
      "Epoch 6500 / 10000, training cost: 3.2672\n",
      "Epoch 6600 / 10000, training cost: 3.26697\n",
      "Epoch 6700 / 10000, training cost: 3.26214\n",
      "Epoch 6800 / 10000, training cost: 3.26197\n",
      "Epoch 6900 / 10000, training cost: 3.25484\n",
      "Epoch 7000 / 10000, training cost: 3.25584\n",
      "Epoch 7100 / 10000, training cost: 3.2547\n",
      "Epoch 7200 / 10000, training cost: 3.25408\n",
      "Epoch 7300 / 10000, training cost: 3.25518\n",
      "Epoch 7400 / 10000, training cost: 3.25497\n",
      "Epoch 7500 / 10000, training cost: 3.25445\n",
      "Epoch 7600 / 10000, training cost: 3.25442\n",
      "Epoch 7700 / 10000, training cost: 3.25427\n",
      "Epoch 7800 / 10000, training cost: 3.25435\n",
      "Epoch 7900 / 10000, training cost: 3.25359\n",
      "Epoch 8000 / 10000, training cost: 3.25465\n",
      "Epoch 8100 / 10000, training cost: 3.25373\n",
      "Epoch 8200 / 10000, training cost: 3.25356\n",
      "Epoch 8300 / 10000, training cost: 3.25426\n",
      "Epoch 8400 / 10000, training cost: 3.25447\n",
      "Epoch 8500 / 10000, training cost: 3.25505\n",
      "Epoch 8600 / 10000, training cost: 3.25353\n",
      "Epoch 8700 / 10000, training cost: 3.25433\n",
      "Epoch 8800 / 10000, training cost: 3.25344\n",
      "Epoch 8900 / 10000, training cost: 3.25325\n",
      "Epoch 9000 / 10000, training cost: 3.25401\n",
      "Epoch 9100 / 10000, training cost: 3.25362\n",
      "Epoch 9200 / 10000, training cost: 3.2541\n",
      "Epoch 9300 / 10000, training cost: 3.24043\n",
      "Epoch 9400 / 10000, training cost: 3.23851\n",
      "Epoch 9500 / 10000, training cost: 3.23132\n",
      "Epoch 9600 / 10000, training cost: 3.23003\n",
      "Epoch 9700 / 10000, training cost: 3.23031\n",
      "Epoch 9800 / 10000, training cost: 3.22345\n",
      "Epoch 9900 / 10000, training cost: 3.22295\n",
      "Epoch 10000 / 10000, training cost: 3.22208\n",
      "Distance between these two sentences is 765\n",
      "\u001b[1;31;40m \n",
      "Deep learning (also known as deep structured learning or hierarchical learning)\n",
      "is part of a broader family of machine learning methods based on learning data\n",
      "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
      "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
      "processing and communication patterns in a biological nervous system, such as neural\n",
      "coding that attempts to define a relationship between various stimuli and associated\n",
      "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
      "networks, deep belief networks and recurrent neural networks have been applied to\n",
      "fields including computer vision, speech recognition, natural language processing,\n",
      "audio recognition, social network filtering, machine translation, bioinformatics\n",
      "and drug design,[5] where they have produced results comparable to and in some\n",
      "cases superior[6] to human experts.\n",
      " \u001b[0m\n",
      "Deer an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in xrange(1, 1 + training_epochs):\n",
    "        _, cost = session.run([optimizer, loss_op],\n",
    "                              feed_dict={X: x_data, Y: y_data})\n",
    "        if i % 1000 == 0:\n",
    "            print('Epoch %s / %s, training cost: %s' % (i, training_epochs, cost))\n",
    "\n",
    "    context_idxs = [word2ind['D'], word2ind['e']]\n",
    "    logue = context_idxs\n",
    "    for i in xrange(data_num):\n",
    "        y_ = y_pred_max.eval({X: [context_idxs], Y: y_data})[0, 0]\n",
    "        logue.append(y_)\n",
    "        context_idxs = logue[-2:]\n",
    "\n",
    "    sentence = ' '.join(sentence)\n",
    "    pred_sentence = ' '.join([ind2word[i] for i in logue])\n",
    "\n",
    "    import editdistance\n",
    "\n",
    "    print('Distance between these two sentences is %s' % (editdistance.eval(sentence, pred_sentence)))\n",
    "    print(\"\\033[1;31;40m %s \\033[0m\" % (sentence))\n",
    "    print(pred_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
