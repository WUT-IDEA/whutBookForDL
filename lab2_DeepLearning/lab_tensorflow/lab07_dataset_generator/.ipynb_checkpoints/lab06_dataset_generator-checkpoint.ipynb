{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data_generator(num=100):\n",
    "    # define make direction func\n",
    "    def mkdir(name=None):\n",
    "        if os.path.exists(name):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(name)\n",
    "\n",
    "    # generate images\n",
    "    mkdir(name='images')\n",
    "    for i in xrange(num):\n",
    "        new_array = np.zeros(shape=(10, 10), dtype=np.float32) * num\n",
    "        np.save(file='images/%s.npy' % i, arr=new_array)\n",
    "    # generate text\n",
    "    mkdir(name='text')\n",
    "    for i in xrange(num):\n",
    "        new_text = str(i)\n",
    "        with open('text/%s.txt' % i, mode='wb') as text_buffer:\n",
    "            text_buffer.write(new_text)\n",
    "\n",
    "\n",
    "fake_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['images/0.npy' 'text/0.txt']\n",
      " ['images/1.npy' 'text/1.txt']\n",
      " ['images/10.npy' 'text/10.txt']\n",
      " ['images/11.npy' 'text/11.txt']\n",
      " ['images/12.npy' 'text/12.txt']\n",
      " ['images/13.npy' 'text/13.txt']\n",
      " ['images/14.npy' 'text/14.txt']\n",
      " ['images/15.npy' 'text/15.txt']\n",
      " ['images/16.npy' 'text/16.txt']\n",
      " ['images/17.npy' 'text/17.txt']]\n",
      "[['images/18.npy' 'text/18.txt']\n",
      " ['images/19.npy' 'text/19.txt']\n",
      " ['images/2.npy' 'text/2.txt']\n",
      " ['images/20.npy' 'text/20.txt']\n",
      " ['images/21.npy' 'text/21.txt']\n",
      " ['images/22.npy' 'text/22.txt']\n",
      " ['images/23.npy' 'text/23.txt']\n",
      " ['images/24.npy' 'text/24.txt']\n",
      " ['images/25.npy' 'text/25.txt']\n",
      " ['images/26.npy' 'text/26.txt']]\n",
      "[['images/90.npy' 'text/90.txt']\n",
      " ['images/91.npy' 'text/91.txt']\n",
      " ['images/92.npy' 'text/92.txt']\n",
      " ['images/93.npy' 'text/93.txt']\n",
      " ['images/94.npy' 'text/94.txt']\n",
      " ['images/95.npy' 'text/95.txt']\n",
      " ['images/96.npy' 'text/96.txt']\n",
      " ['images/97.npy' 'text/97.txt']\n",
      " ['images/98.npy' 'text/98.txt']\n",
      " ['images/99.npy' 'text/99.txt']]\n",
      "[['images/63.npy' 'text/63.txt']\n",
      " ['images/64.npy' 'text/64.txt']\n",
      " ['images/65.npy' 'text/65.txt']\n",
      " ['images/66.npy' 'text/66.txt']\n",
      " ['images/67.npy' 'text/67.txt']\n",
      " ['images/68.npy' 'text/68.txt']\n",
      " ['images/69.npy' 'text/69.txt']\n",
      " ['images/7.npy' 'text/7.txt']\n",
      " ['images/70.npy' 'text/70.txt']\n",
      " ['images/71.npy' 'text/71.txt']]\n",
      "[['images/27.npy' 'text/27.txt']\n",
      " ['images/28.npy' 'text/28.txt']\n",
      " ['images/29.npy' 'text/29.txt']\n",
      " ['images/3.npy' 'text/3.txt']\n",
      " ['images/30.npy' 'text/30.txt']\n",
      " ['images/31.npy' 'text/31.txt']\n",
      " ['images/32.npy' 'text/32.txt']\n",
      " ['images/33.npy' 'text/33.txt']\n",
      " ['images/34.npy' 'text/34.txt']\n",
      " ['images/35.npy' 'text/35.txt']]\n",
      "[['images/54.npy' 'text/54.txt']\n",
      " ['images/55.npy' 'text/55.txt']\n",
      " ['images/56.npy' 'text/56.txt']\n",
      " ['images/57.npy' 'text/57.txt']\n",
      " ['images/58.npy' 'text/58.txt']\n",
      " ['images/59.npy' 'text/59.txt']\n",
      " ['images/6.npy' 'text/6.txt']\n",
      " ['images/60.npy' 'text/60.txt']\n",
      " ['images/61.npy' 'text/61.txt']\n",
      " ['images/62.npy' 'text/62.txt']]\n",
      "[['images/36.npy' 'text/36.txt']\n",
      " ['images/37.npy' 'text/37.txt']\n",
      " ['images/38.npy' 'text/38.txt']\n",
      " ['images/39.npy' 'text/39.txt']\n",
      " ['images/4.npy' 'text/4.txt']\n",
      " ['images/40.npy' 'text/40.txt']\n",
      " ['images/41.npy' 'text/41.txt']\n",
      " ['images/42.npy' 'text/42.txt']\n",
      " ['images/43.npy' 'text/43.txt']\n",
      " ['images/44.npy' 'text/44.txt']]\n",
      "[['images/45.npy' 'text/45.txt']\n",
      " ['images/46.npy' 'text/46.txt']\n",
      " ['images/47.npy' 'text/47.txt']\n",
      " ['images/48.npy' 'text/48.txt']\n",
      " ['images/49.npy' 'text/49.txt']\n",
      " ['images/5.npy' 'text/5.txt']\n",
      " ['images/50.npy' 'text/50.txt']\n",
      " ['images/51.npy' 'text/51.txt']\n",
      " ['images/52.npy' 'text/52.txt']\n",
      " ['images/53.npy' 'text/53.txt']]\n",
      "[['images/72.npy' 'text/72.txt']\n",
      " ['images/73.npy' 'text/73.txt']\n",
      " ['images/74.npy' 'text/74.txt']\n",
      " ['images/75.npy' 'text/75.txt']\n",
      " ['images/76.npy' 'text/76.txt']\n",
      " ['images/77.npy' 'text/77.txt']\n",
      " ['images/78.npy' 'text/78.txt']\n",
      " ['images/79.npy' 'text/79.txt']\n",
      " ['images/8.npy' 'text/8.txt']\n",
      " ['images/80.npy' 'text/80.txt']]\n",
      "[['images/81.npy' 'text/81.txt']\n",
      " ['images/82.npy' 'text/82.txt']\n",
      " ['images/83.npy' 'text/83.txt']\n",
      " ['images/84.npy' 'text/84.txt']\n",
      " ['images/85.npy' 'text/85.txt']\n",
      " ['images/86.npy' 'text/86.txt']\n",
      " ['images/87.npy' 'text/87.txt']\n",
      " ['images/88.npy' 'text/88.txt']\n",
      " ['images/89.npy' 'text/89.txt']\n",
      " ['images/9.npy' 'text/9.txt']]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow 1.4 on, shoud \"from tensorflow.data import Dataset\"\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "img_dir = 'images'\n",
    "text_dir = 'text'\n",
    "\n",
    "imgs = [os.path.join(img_dir, i) for i in sorted(os.listdir(img_dir))]\n",
    "text = [os.path.join(text_dir, i) for i in sorted(os.listdir(text_dir))]\n",
    "files = tf.constant(zip(imgs, text))\n",
    "\n",
    "# 一般来说，batch_size越小，模型训练效果越快，而且越好；但是耗时长。\n",
    "batch_size = 10\n",
    "num = len(os.listdir(img_dir))\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(files)\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "# dataset.repeat的count为None时，可以无限循环\n",
    "dataset = dataset.repeat(count=None)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "x = iterator.get_next()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(iterator.initializer)\n",
    "    for _ in xrange(num // batch_size):\n",
    "        print(session.run(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
