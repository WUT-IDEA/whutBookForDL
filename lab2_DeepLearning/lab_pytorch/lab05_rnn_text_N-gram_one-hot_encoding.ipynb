{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归神经网络 - Recurrent Neural Network\n",
    "====\n",
    ">Python2.7 + Pytorch 1.2.0 backened\n",
    ">\n",
    ">text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节主要参考：\n",
    "http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7effd859e330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.N-Gram Language Modeling <br>\n",
    "probability{ x_(i-N),x_(i-N+1),x_(i-N+2),...,x_(i-1) -> x_(i) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"\n",
    "Deep learning (also known as deep structured learning or hierarchical learning)\n",
    "is part of a broader family of machine learning methods based on learning data\n",
    "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
    "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
    "processing and communication patterns in a biological nervous system, such as neural\n",
    "coding that attempts to define a relationship between various stimuli and associated\n",
    "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
    "networks, deep belief networks and recurrent neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing,\n",
    "audio recognition, social network filtering, machine translation, bioinformatics\n",
    "and drug design,[5] where they have produced results comparable to and in some\n",
    "cases superior[6] to human experts.\n",
    "\"\"\"\n",
    "# from wikipedia https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word2ind = {word: i for i, word in enumerate(vocab)}\n",
    "ind2word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "x_length = 2\n",
    "num_classes = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoding\n",
    "def one_hot(x, num_classes=num_classes):\n",
    "    x_one_hot = torch.LongTensor(x).view(-1, 1)\n",
    "    x_one_hot = torch.zeros(x_one_hot.size(0), num_classes).scatter_(1, x_one_hot, 1)\n",
    "    return x_one_hot\n",
    "\n",
    "data_num = len(test_sentence) - x_length\n",
    "x = [[word2ind[ch] for ch in test_sentence[i:i + x_length]]\n",
    "          for i in xrange(data_num)]\n",
    "x = one_hot(x)\n",
    "x = x.view([-1, x_length, num_classes])\n",
    "y = torch.LongTensor([[word2ind[test_sentence[i]]] for i in xrange(x_length, len(test_sentence))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM (\n",
      "  (BiLSTM): LSTM(36, 60, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear (240 -> 36)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "training_epoch = 10000\n",
    "learning_rate = 1e-3\n",
    "input_size = len(vocab)\n",
    "hidden_size = 60\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "# basic RNN/GRU\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_class = num_classes\n",
    "\n",
    "        self.RNN = nn.RNN(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        # self.GRU = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "        #                   num_layers=num_layers, batch_first=True)\n",
    "        # '*2' for 2 sequences -> 1 sequences\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "        out, _ = self.RNN(x, h_0)\n",
    "        # out, _ = self.GRU(x, h_0)\n",
    "        out = out.contiguous().view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_class = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        # '*2' for 2 sequences -> 1 sequences\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        out, _ = self.LSTM(x, (h_0, c_0))\n",
    "        out = out.contiguous().view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# BiLSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_class = num_classes\n",
    "\n",
    "        self.BiLSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                              num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        # '*2' for 2 sequences -> 1 sequences\n",
    "        # '*2' for bidirectional RNN\n",
    "        self.fc = nn.Linear(hidden_size * 2 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "        c_0 = Variable(torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size)).cuda()\n",
    "\n",
    "        out, _ = self.BiLSTM(x, (h_0, c_0))\n",
    "        out = out.contiguous().view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = BiLSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "               num_layers=num_layers, num_classes=num_classes)\n",
    "model.cuda()\n",
    "print(model)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 / 10000, training cost: 2.89640688896\n",
      "Epoch 2000 / 10000, training cost: 2.52059006691\n",
      "Epoch 3000 / 10000, training cost: 2.28343462944\n",
      "Epoch 4000 / 10000, training cost: 2.12130451202\n",
      "Epoch 5000 / 10000, training cost: 1.9828094244\n",
      "Epoch 6000 / 10000, training cost: 1.8586704731\n",
      "Epoch 7000 / 10000, training cost: 1.74536836147\n",
      "Epoch 8000 / 10000, training cost: 1.6385037899\n",
      "Epoch 9000 / 10000, training cost: 1.53528344631\n",
      "Epoch 10000 / 10000, training cost: 1.43692815304\n"
     ]
    }
   ],
   "source": [
    "x = Variable(x).cuda()\n",
    "y = Variable(y).cuda()\n",
    "\n",
    "for epoch in xrange(1, 1 + training_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    y_ = model(x)\n",
    "    loss = criterion(y_, y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print('Epoch %s / %s, training cost: %s' % (epoch, training_epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating text\n",
    "model.eval()\n",
    "context_idxs = [word2ind['D'], word2ind['e']]\n",
    "logue = context_idxs\n",
    "for i in xrange(data_num):\n",
    "    context_var = Variable(one_hot([context_idxs]).view([-1, x_length, num_classes])).cuda()\n",
    "    context_idxs = model(context_var).data.topk(1)[1].cpu().numpy()[0, 0]\n",
    "    logue.append(context_idxs)\n",
    "    context_idxs = logue[-2:]\n",
    "\n",
    "pred_sentence = ''.join([ind2word[i] for i in logue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between these two sentences is 693\n",
      "\u001b[1;31;40m \n",
      "Deep learning (also known as deep structured learning or hierarchical learning)\n",
      "is part of a broader family of machine learning methods based on learning data\n",
      "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
      "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
      "processing and communication patterns in a biological nervous system, such as neural\n",
      "coding that attempts to define a relationship between various stimuli and associated\n",
      "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
      "networks, deep belief networks and recurrent neural networks have been applied to\n",
      "fields including computer vision, speech recognition, natural language processing,\n",
      "audio recognition, social network filtering, machine translation, bioinformatics\n",
      "and drug design,[5] where they have produced results comparable to and in some\n",
      "cases superior[6] to human experts.\n",
      " \u001b[0m\n",
      "Deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to deep learning and to d\n"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "\n",
    "print('Distance between these two sentences is %s' % (editdistance.eval(test_sentence, pred_sentence)))\n",
    "print(\"\\033[1;31;40m %s \\033[0m\" % (test_sentence))\n",
    "print(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
